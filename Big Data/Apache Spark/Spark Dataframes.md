# Spark and PySpark

Apache Spark is a unified analytics engine for large-scale data processing.
It lets you write applications in Java, Scala, Python, R, and SQL and runs on
Hadoop, stand-alone, or in the cloud (and many other platforms
* Uses a Resilient Distributed Dataset (RDD) as its basic abstraction layer
  * RDD is an immutable, partitioned collection of elements **that can be operated on in parallel**

